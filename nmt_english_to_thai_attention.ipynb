{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Attention: Thai to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random, math, time\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.1+cpu\n",
      "Torchtext version: 0.16.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchtext version:\", torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('Tsunnami/who-en-th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'th'],\n",
      "        num_rows: 538\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'th'\n",
    "\n",
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'th'],\n",
       "    num_rows: 538\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Tobacco and nicotine industry tactics addict youth for life',\n",
       " 'th': 'กลยุทธ์ของอุตสาหกรรมยาสูบและนิโคตินทำให้เยาวชนเสพติดไปชั่วชีวิต'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total size of the dataset\n",
    "total_size = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "# 80% for training, 10% for validation, \n",
    "# and 10% for testing to maximize training data for a small dataset.\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  # Remaining for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 430\n",
      "Validation size: 54\n",
      "Test size: 54\n"
     ]
    }
   ],
   "source": [
    "# First split: Split into 80% train and 20% (val + test)\n",
    "train, temp = train.train_test_split(test_size=0.2, seed=999).values()\n",
    "\n",
    "# Second split: Split the remaining 20% into 50% val and 50% test (10% each)\n",
    "val, test = temp.train_test_split(test_size=0.5, seed=999).values()\n",
    "\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Validation size: {len(val)}\")\n",
    "print(f\"Test size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Tokenizer for English (using spaCy)\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Tokenizer for Thai (using PyThaiNLP for Thai segmentation)\n",
    "from pythainlp import word_tokenize\n",
    "token_transform[TRG_LANGUAGE] = word_tokenize  # PyThaiNLP tokenizer for Thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence (English):  Tobacco and nicotine industry tactics addict youth for life\n",
      "Tokenization (English):  ['Tobacco', 'and', 'nicotine', 'industry', 'tactics', 'addict', 'youth', 'for', 'life']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the source language (English)\n",
    "english_sentence = sample['en']\n",
    "print(\"Sentence (English): \", english_sentence)\n",
    "tokenized_english = token_transform[SRC_LANGUAGE](english_sentence)\n",
    "print(\"Tokenization (English): \", tokenized_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence (Thai):  กลยุทธ์ของอุตสาหกรรมยาสูบและนิโคตินทำให้เยาวชนเสพติดไปชั่วชีวิต\n",
      "Tokenization (Thai):  ['กลยุทธ์', 'ของ', 'อุตสาหกรรม', 'ยาสูบ', 'และ', 'นิโคติน', 'ทำให้', 'เยาวชน', 'เสพติด', 'ไป', 'ชั่วชีวิต']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the target language (Thai)\n",
    "thai_sentence = sample['th']\n",
    "print(\"Sentence (Thai): \", thai_sentence)\n",
    "tokenized_thai = token_transform[TRG_LANGUAGE](thai_sentence)\n",
    "print(\"Tokenization (Thai): \", tokenized_thai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to yield list of tokens\n",
    "# Here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 'en', TRG_LANGUAGE: 'th'}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]])  # either 'en' or 'th'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Build vocabulary for both source (English) and target (Thai) languages\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train, ln),\n",
    "        min_freq=2,  # If a token appears less than twice, it will be treated as UNK\n",
    "        specials=special_symbols,\n",
    "        special_first=True  # Insert special symbols at the beginning\n",
    "    )\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokenization (English): [0, 17, 12, 0, 12]\n"
     ]
    }
   ],
   "source": [
    "# Check some examples in the vocabulary\n",
    "print(\"Example tokenization (English):\", vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping index 22 to word: The\n"
     ]
    }
   ],
   "source": [
    "# Reverse vocabulary lookup: get the token by its index\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "# For example, get the word corresponding to index 22\n",
    "print(\"Mapping index 22 to word:\", mapping[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token for <unk>: <unk>\n"
     ]
    }
   ],
   "source": [
    "# Looking up the special token for unknown words\n",
    "print(\"Special token for <unk>:\", mapping[0])  # All unknown words will map to <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token for <pad>: <pad>\n",
      "Special token for <sos>: <sos>\n",
      "Special token for <eos>: <eos>\n"
     ]
    }
   ],
   "source": [
    "# Looking up other special tokens\n",
    "print(\"Special token for <pad>:\", mapping[1])\n",
    "print(\"Special token for <sos>:\", mapping[2])\n",
    "print(\"Special token for <eos>:\", mapping[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1135\n"
     ]
    }
   ],
   "source": [
    "# Check how many unique words are in the English vocabulary\n",
    "print(f\"Vocabulary size: {len(mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16 # Better for small datasets and faster updates\n",
    "\n",
    "# Helper function to apply sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensor indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(\n",
    "        token_transform[ln], # Tokenization\n",
    "        vocab_transform[ln], # Numericalization\n",
    "        tensor_transform     # Add BOS/EOS and create tensor\n",
    "    )\n",
    "# Function to collate data samples into batch tensors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    \n",
    "    for sample in batch:\n",
    "        # Make sure to access the correct fields\n",
    "        src_sample = sample['en'] \n",
    "        trg_sample = sample['th']\n",
    "        \n",
    "        # Apply text transformations (tokenization, vocab conversion, tensor creation)\n",
    "        processed_src_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        processed_trg_text = text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\"))\n",
    "        \n",
    "        src_batch.append(processed_src_text)\n",
    "        trg_batch.append(processed_trg_text)\n",
    "        src_len_batch.append(processed_src_text.size(0))\n",
    "\n",
    "    # Pad sequences in the batch\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    \n",
    "    # Return the batch along with source sequence lengths\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders for training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check the shape of a batch from the training data loader\n",
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([16, 37])\n",
      "Thai shape:  torch.Size([16, 38])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq_len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device  = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch_size]\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so that it's the same shape as attention\n",
    "        #mask: [batch_size, src len] #(0, 0, 0, 0, 0, 1, 1)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src: [src len, batch_size]\n",
    "        #trg: [trg len, batch_size]\n",
    "        \n",
    "        #initialize something\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len    = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs    = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #send our src text into encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        #encoder_outputs refer to all hidden states (last layer)\n",
    "        #hidden refer to the last hidden state (of each layer, of each direction)\n",
    "        \n",
    "        input_ = trg[0, :]\n",
    "        \n",
    "        mask   = self.create_mask(src) #(0, 0, 0, 0, 0, 1, 1)\n",
    "        \n",
    "        #for each of the input of the trg text\n",
    "        for t in range(1, trg_len):\n",
    "            #send them to the decoder\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output: [batch_size, output_dim] ==> predictions\n",
    "            #hidden: [batch_size, hid_dim]\n",
    "            #attention: [batch_size, src len]\n",
    "            \n",
    "            #append the output to a list\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1          = output.argmax(1)  #autoregressive\n",
    "            \n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        #embedding\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #packed\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "        #rnn\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        #unpacked\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #-1, -2 hidden state\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim = 1)))\n",
    "        \n",
    "        #outputs: [src len, batch_size, hid dim * 2]\n",
    "        #hidden:  [batch_size, hid_dim]\n",
    "        \n",
    "        return outputs, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention  \n",
    "\n",
    "General Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralAttention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hid_dim, hid_dim)  # for decoder input\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim)  # for encoder outputs\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)  # for final score\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hid_dim * 2]\n",
    "        \n",
    "        energy = self.v(torch.tanh(self.W(hidden) + self.U(encoder_outputs))).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        energy = energy.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(energy, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicative Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hid_dim, hid_dim, bias=False)  # for decoder input\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim, bias=False)  # for encoder outputs\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hid_dim * 2]\n",
    "        \n",
    "        # Scaling dot product (multiplicative attention)\n",
    "        energy = torch.bmm(hidden, encoder_outputs.permute(0, 2, 1))  # [batch_size, src_len, src_len]\n",
    "        energy = energy.squeeze(1)  # [batch_size, src_len]\n",
    "        \n",
    "        energy = energy.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(energy, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "        self.W = nn.Linear(hid_dim, hid_dim)  # for decoder input\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim)  # for encoder outputs\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hid_dim * 2]\n",
    "        \n",
    "        energy = self.v(torch.tanh(self.W(hidden) + self.U(encoder_outputs))).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        energy = energy.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(energy, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention  = attention\n",
    "        self.embedding  = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn        = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc         = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        #input: [batch_size]\n",
    "        #hidden: [batch_size, hid_dim]\n",
    "        #encoder_ouputs: [src len, batch_size, hid_dim * 2]\n",
    "        #mask: [batch_size, src len]\n",
    "                \n",
    "        #embed our input\n",
    "        input    = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        #calculate the attention\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch_size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch_size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_ouputs: [batch_size, src len, hid_dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted: [batch_size, 1, hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted: [1, batch_size, hid_dim * 2]\n",
    "        \n",
    "        #send the input to decoder rnn\n",
    "            #concatenate (embed, weighted encoder_outputs)\n",
    "            #[1, batch_size, emb_dim]; [1, batch_size, hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input: [1, batch_size, emb_dim + hid_dim * 2]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "            \n",
    "        #send the output of the decoder rnn to fc layer to predict the word\n",
    "            #prediction = fc(concatenate (output, weighted, embed))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc(torch.cat((embedded, output, weighted), dim = 1))\n",
    "        #prediction: [batch_size, output_dim]\n",
    "            \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(1135, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(1089, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=1089, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn_general = GeneralAttention(hid_dim)\n",
    "attn_multiplicative = MultiplicativeAttention(hid_dim)\n",
    "attn_additive = AdditiveAttention(hid_dim)\n",
    "\n",
    "enc  = Encoder(input_dim, emb_dim, hid_dim, dropout)\n",
    "\n",
    "dec_general = Decoder(output_dim, emb_dim, hid_dim, dropout, attn_general)\n",
    "dec_multiplicative = Decoder(output_dim, emb_dim, hid_dim, dropout, attn_multiplicative)\n",
    "dec_additive = Decoder(output_dim, emb_dim, hid_dim, dropout, attn_additive)\n",
    "\n",
    "model_general = Seq2SeqPackedAttention(enc, dec_general, SRC_PAD_IDX, device).to(device)\n",
    "model_multiplicative = Seq2SeqPackedAttention(enc, dec_multiplicative, SRC_PAD_IDX, device).to(device)\n",
    "model_additive = Seq2SeqPackedAttention(enc, dec_additive, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "model_general.apply(initialize_weights)\n",
    "model_multiplicative.apply(initialize_weights)\n",
    "model_additive.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290560\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "278784\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "1951488\n",
      "  1089\n",
      "______\n",
      "8955713\n",
      "290560\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "262144\n",
      "524288\n",
      "278784\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "1951488\n",
      "  1089\n",
      "______\n",
      "8954177\n",
      "290560\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "278784\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "1951488\n",
      "  1089\n",
      "______\n",
      "8955713\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model_general)\n",
    "count_parameters(model_multiplicative)\n",
    "count_parameters(model_additive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# training hyperparameters\n",
    "optimizer_general = optim.Adam(model_general.parameters(), lr=lr)\n",
    "optimizer_multiplicative = optim.Adam(model_multiplicative.parameters(), lr=lr)\n",
    "optimizer_additive = optim.Adam(model_additive.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_length, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss_general = float('inf')\n",
    "best_valid_loss_multiplicative = float('inf')\n",
    "best_valid_loss_additive = float('inf')\n",
    "\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path_general = f'models/{model_general.__class__.__name__}.pt'\n",
    "save_path_multiplicative = f'models/{model_multiplicative.__class__.__name__}.pt'\n",
    "save_path_additive = f'models/{model_additive.__class__.__name__}.pt'\n",
    "\n",
    "train_losses_general = []\n",
    "valid_losses_general = []\n",
    "\n",
    "train_losses_multiplicative = []\n",
    "valid_losses_multiplicative = []\n",
    "\n",
    "train_losses_additive = []\n",
    "valid_losses_additive = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss_general = train(model_general, train_loader, optimizer_general, criterion, clip, train_loader_length)\n",
    "    valid_loss_general = evaluate(model_general, valid_loader, criterion, val_loader_length)\n",
    "    train_losses_general.append(train_loss_general)\n",
    "    valid_losses_general.append(valid_loss_general)\n",
    "    \n",
    "    train_loss_multiplicative = train(model_multiplicative, train_loader, optimizer_multiplicative, criterion, clip, train_loader_length)\n",
    "    valid_loss_multiplicative = evaluate(model_multiplicative, valid_loader, criterion, val_loader_length)\n",
    "    train_losses_multiplicative.append(train_loss_multiplicative)\n",
    "    valid_losses_multiplicative.append(valid_loss_multiplicative)\n",
    "    \n",
    "    train_loss_additive = train(model_additive, train_loader, optimizer_additive, criterion, clip, train_loader_length)\n",
    "    valid_loss_additive = evaluate(model_additive, valid_loader, criterion, val_loader_length)\n",
    "    train_losses_additive.append(train_loss_additive)\n",
    "    valid_losses_additive.append(valid_loss_additive)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss_general < best_valid_loss_general:\n",
    "        best_valid_loss_general = valid_loss_general\n",
    "        torch.save(model_general.state_dict(), save_path_general)\n",
    "        \n",
    "    if valid_loss_multiplicative < best_valid_loss_multiplicative:\n",
    "        best_valid_loss_multiplicative = valid_loss_multiplicative\n",
    "        torch.save(model_multiplicative.state_dict(), save_path_multiplicative)\n",
    "        \n",
    "    if valid_loss_additive < best_valid_loss_additive:\n",
    "        best_valid_loss_additive = valid_loss_additive\n",
    "        torch.save(model_additive.state_dict(), save_path_additive)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tGeneral Model - Train Loss: {train_loss_general:.3f} | Train PPL: {math.exp(train_loss_general):7.3f}')\n",
    "    print(f'\\tGeneral Model - Val. Loss: {valid_loss_general:.3f} |  Val. PPL: {math.exp(valid_loss_general):7.3f}')\n",
    "    \n",
    "    print(f'\\tMultiplicative Model - Train Loss: {train_loss_multiplicative:.3f} | Train PPL: {math.exp(train_loss_multiplicative):7.3f}')\n",
    "    print(f'\\tMultiplicative Model - Val. Loss: {valid_loss_multiplicative:.3f} |  Val. PPL: {math.exp(valid_loss_multiplicative):7.3f}')\n",
    "    \n",
    "    print(f'\\tAdditive Model - Train Loss: {train_loss_additive:.3f} | Train PPL: {math.exp(train_loss_additive):7.3f}')\n",
    "    print(f'\\tAdditive Model - Val. Loss: {valid_loss_additive:.3f} |  Val. PPL: {math.exp(valid_loss_additive):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Training and Validation Losses for Each Attention Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure for plotting\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "# Plot for General Attention Model\n",
    "axs[0].plot(train_losses_general, label='train loss')\n",
    "axs[0].plot(valid_losses_general, label='valid loss')\n",
    "axs[0].set_title('General Attention')\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "# Plot for Multiplicative Attention Model\n",
    "axs[1].plot(train_losses_multiplicative, label='train loss')\n",
    "axs[1].plot(valid_losses_multiplicative, label='valid loss')\n",
    "axs[1].set_title('Multiplicative Attention')\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "\n",
    "# Plot for Additive Attention Model\n",
    "axs[2].plot(train_losses_additive, label='train loss')\n",
    "axs[2].plot(valid_losses_additive, label='valid loss')\n",
    "axs[2].set_title('Additive Attention')\n",
    "axs[2].legend()\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and Test the Best Model for Each Attention Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For General Attention Model\n",
    "model_general.load_state_dict(torch.load(save_path_general))\n",
    "test_loss_general = evaluate(model_general, test_loader, criterion, test_loader_length)\n",
    "print(f'General Model - Test Loss: {test_loss_general:.3f} | Test PPL: {math.exp(test_loss_general):7.3f}')\n",
    "\n",
    "# For Multiplicative Attention Model\n",
    "model_multiplicative.load_state_dict(torch.load(save_path_multiplicative))\n",
    "test_loss_multiplicative = evaluate(model_multiplicative, test_loader, criterion, test_loader_length)\n",
    "print(f'Multiplicative Model - Test Loss: {test_loss_multiplicative:.3f} | Test PPL: {math.exp(test_loss_multiplicative):7.3f}')\n",
    "\n",
    "# For Additive Attention Model\n",
    "model_additive.load_state_dict(torch.load(save_path_additive))\n",
    "test_loss_additive = evaluate(model_additive, test_loader, criterion, test_loader_length)\n",
    "print(f'Additive Model - Test Loss: {test_loss_additive:.3f} | Test PPL: {math.exp(test_loss_additive):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU score shows how good a translation is by comparing it to a reference translation. A higher BLEU score means the translation is better and more similar to the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    return corpus_bleu(reference, hypothesis)\n",
    "\n",
    "# Evaluate BLEU score for each attention mechanism after translation\n",
    "def evaluate_bleu_score(models, test_loader):\n",
    "    all_hypotheses = {model.__class__.__name__: [] for model in models}\n",
    "    all_references = []\n",
    "\n",
    "    for src, src_length, trg in test_loader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "\n",
    "            output, _ = model(src, src_length, trg, 0)  # No teacher forcing during eval\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "            # Collect predictions for BLEU score calculation\n",
    "            for batch_idx, pred in enumerate(output):\n",
    "                hypothesis = [vocab_transform[TRG_LANGUAGE].lookup_tokens(pred)]\n",
    "                reference = [vocab_transform[TRG_LANGUAGE].lookup_tokens(trg[batch_idx].cpu().numpy())]\n",
    "                all_hypotheses[model.__class__.__name__].append(hypothesis)\n",
    "                all_references.append(reference)\n",
    "\n",
    "    bleu_scores = {}\n",
    "    for model_name, hypotheses in all_hypotheses.items():\n",
    "        bleu_scores[model_name] = calculate_bleu_score(all_references, hypotheses)\n",
    "    \n",
    "    return bleu_scores\n",
    "\n",
    "# Example of using BLEU scores\n",
    "bleu_scores = evaluate_bleu_score([model_general, model_multiplicative, model_additive], test_loader)\n",
    "print(bleu_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(-1, 1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_general.load_state_dict(torch.load(save_path_general))\n",
    "\n",
    "model_general.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model_general(src_text, text_length, trg_text, 0) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape #trg_len, batch_size, trg_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices\n",
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Attention Maps for Each Attention Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention, attention_type):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "    \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks = [''] + translation\n",
    "    x_ticks = [''] + sentence\n",
    "    \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    ax.set_title(f'{attention_type} Attention')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src_tokens, trg_tokens, attentions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
